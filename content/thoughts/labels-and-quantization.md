---
title: "Labels and Quantization"
date: 2021-05-04T22:22:23-04:00
---

> "Where is the knowledge we have lost in the information?" -- T.S. Eliot's "The Rock"

Our obsession of applying labels to everything extends to even whether a [hot dog is a sandwhich or not](https://www.hot-dog.org/culture/hot-dog-sandwich)

"Accuracy is more useful in entry-level jobs and for novices, because as skill increases, quantification of skill becomes harder."

Why do we have labels in the first place?
- they help us to communicate complex ideas between each other without having to explain our entire mental models
- they are attached to societal connotations and perceptions of certain concepts
- they give legitimacy in the form of social proof to concepts

In the context of [hackathons](thoughts/hackathons.md), we spend a lot of time clearing up that hackathons don't involve hacking devices, but rather creating creative and innovative solutions

Are overloaded terms still useful? (e.g. [hacker](thoughts/books/hackers.md) has so many connotations attached to it) At that point, do we need to create new terms?

**Hermeneutical injustice** is a subcategory of epistemic injustice wherein one has no labels/common terminology to describe or explain experiences to others. Historically has been applied in the context of exclusion of marginalized groups from activities which shape the language we use.

## [Feedback Loops](thoughts/feedback-loops.md)
1. Practice creates new terminology as a way to communicate complex ideas without needing to rexplain each time
2. Terminology then shapes how we think about the world: [Sapir-Whorf](thoughts/language-of-thought.md)
3. Some terminology becomes outdated as practice changes
4. Arguments ensue over updating shared terminology (esp as language is decentralized, this can cause fracturing)
5. Two camps emerge
	1. People who want to use terminology mainly as a means to more efficiently communicate practice
	2. People who like the theory behind terminology or those who are attached to tradition
6. Camps argue over who has the 'right' definitions but for their own goals
	1. Camp 1 repeats this loop
	2. Camp 2 continues to argue over definitions

"This is because feedback loops which are too short for the overall system makes people focus on inappropriate intermediate goals."
* dangers of intermediate proxies?
* what about in machine learning? using certain quantities for metrics which are just *proxies* for more difficult to measure qualitative end goals

Applicable to [feedback-loops](/thoughts/feedback-loops)

"When quantifying things, people naturally focus on things that can easily be measured. Measuring the final result doesn’t provide enough quantitative data, so it’s tempting to include the data from intermediate steps. This is an attempt to shorten the feedback loop, and trying to shorten feedback loops is very dangerous in complex systems."

## [Qualia](thoughts/qualia.md)
Is there any anyway to label or quantify the subjective human experience? Probably not.

If the same apple sends two very different signals to two different people's brains, how is it that we decipher it to be [semantically](thoughts/semantics.md) identical?

Not sure if there's any way to easily do this

## Nonsemantic Information
"[the] shadows, wind, rust, in the signs of wear on a well-trodden staircase, the creaks of a battered bridge — all the indexical messages of our material environments" From [A city is not a computer](thoughts/articles/a-city-is-not-a-computer.md)

## Phrases
### Goodhart's Law
> "When a measure becomes a target, it ceases to be a good measure" -- [Goodhart's Law](https://en.wikipedia.org/wiki/Goodhart%27s_law)

### McNamara Fallacy
Also known as the quantitative fallacy: making a decision involving purely quantitative observations (ignoring all others) is often wrong. [Source](https://en.wikipedia.org/wiki/McNamara_fallacy)

## Are labels helpful? 
In a data-driven world, can we and should we try to quantize everything? What about inherently human qualities like emotions or personality?

some metrics that are inherently v difficult to quantize (e.g. quality of engagement) and others that are more easy to quantize and thus optimize for (like engagement)

https://outline.com/5H8EEy
No matter how much data we collect, two people who look the same to the algorithm can always end up making different choices.

We gave you two definitions of fairness: keep the error rates comparable between groups, and treat people with the same risk scores in the same way. Both of these definitions are totally defensible! But satisfying both at the same time is impossible.

relevant bit on algorithms and algorithmic decision making -> [to-live-in-their-utopia](/thoughts/articles/to-live-in-their-utopia)