---
title: "Emergent Behaviour"
date: 2021-06-17T00:21:11-04:00
---

How complex behaviour can arise out of seemingly simple rules? Is there anything special that causes emergent behaviour?
* Ant simulations
* Mold simulations
* [Community](/thoughts/communities) dynamics

Interesting to think about in context of single agents in multi-agent systems. How does [consciousness](thoughts/consciousness.md) arise? Is it just because of the rules itself (a reductionist approach) or is there something larger at play?

Even in systems of [representation](/thoughts/representation)

## Gall's Law
A rule of thumb for complex system design: simple alphabets produce complex behaviors, complex alphabets produce stupid behaviors

> A complex system that works is invariably found to have evolved from a simple system that worked. A complex system designed from scratch never works and cannot be patched up to make it work. You have to start over with a working simple system

### Measures of Complexity
- [Vapnikâ€“Chervonenkis dimension](https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension): cardinality of the largest set of points that a binary classification algorithm can learn.
- [Kolmogorov complexity](https://en.wikipedia.org/wiki/Kolmogorov_complexity): length of the shortest computer program that produces the object as output.

## Lossiness as Mutation
[Source](https://subconscious.substack.com/p/self-organizing-ideas)

Maybe chaos is necessary for [emergent behaviour](thoughts/emergent-behaviour.md). Thus, lossy communication in low [bandwidth](thoughts/bandwidth.md) communication helps seed for selection/mutation of new ideas. Examples of this include writing.

When we write, **[we flatten the cloud of associated ideas in our head](https://subconscious.substack.com/p/hypertext-montage)** into a linearized subset (lossy). The reader then unflattens this linearized subset into their own cloud of associated ideas (lossy). Each lossy step is an opportunity for **mutations** in understanding to emerge.